{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d106d8c",
   "metadata": {},
   "source": [
    "# Customer‚ÄëSupport Chatbot for an E-Commerce Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2851703",
   "metadata": {},
   "source": [
    "## Learning Objectives  \n",
    "* **Ingest & chunk** unstructured docs  \n",
    "* **Embed** chunks and **index** with *FAISS*  \n",
    "* **Retrieve** context and **craft prompts**  \n",
    "* **Run** an open‚Äëweight LLM locally with *Ollama*  \n",
    "* **Build** a Retrieval-Augmented Generation (RAG) chain\n",
    "* **Package** the chat loop in a minimal **Streamlit** web UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cdcc78",
   "metadata": {},
   "source": [
    "## Roadmap  \n",
    "We will build a RAG-based chatbot in **six** steps:\n",
    "\n",
    "1. **Environment setup**\n",
    "2. **Data preparation**  \n",
    "   a. Load source documents  \n",
    "   b. Chunk the text  \n",
    "3. **Build a retriever**  \n",
    "   a. Generate embeddings  \n",
    "   b. Build a FAISS vector index  \n",
    "4. **Build a generation engine**. Load the *Gemma3-1B* model through Ollama and run a sanity check.  \n",
    "5. **Build a RAG**. Connect the system prompt, retriever, and LLM together. \n",
    "6. **(Optional) Streamlit UI**. Wrap everything in a simple web app so users can chat with the bot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c18870a",
   "metadata": {},
   "source": [
    "## 1‚ÄØ-‚ÄØEnvironment setup\n",
    "\n",
    "We use conda to manage our project dependencies and ensure everyone has a consistent setup. Conda is an open-source package and environment manager that makes it easy to install libraries and switch between isolated environments. To learn more about conda, you can read: https://docs.conda.io/en/latest/\n",
    "\n",
    "Create and activate a clean *conda* environment and install the required packages. If you don't have conda installed, visit https://www.anaconda.com/docs/getting-started/miniconda/main.\n",
    "\n",
    "\n",
    "Open your terminal, navigate to the project folder where this notebook is located, and run the following commands.\n",
    "\n",
    "```bash\n",
    "conda env create -f environment.yml && conda activate rag-chatbot\n",
    "\n",
    "# (Optional but recommended) Register this environment as a Jupyter kernel\n",
    "python -m ipykernel install --user --name=rag-chatbot --display-name \"rag-chatbot\"\n",
    "```\n",
    "Once this is done, you can select ‚Äúrag-chatbot‚Äù from the Kernel ‚Üí Change Kernel menu in Jupyter or VS Code.\n",
    "\n",
    "\n",
    "> Behind the scenes:\n",
    "> * Conda reads `environment.yml`, solves all pinned dependencies, and builds an isolated environment named `rag-chatbot`.\n",
    "> * When it reaches the file‚Äôs \"pip:\" section, Conda automatically invokes pip to install any remaining Python-only packages so the whole stack be available for the project.\n",
    "> * Registering the kernel makes your new environment visible to Jupyter, so the notebook runs inside the same environment you just created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae027b18",
   "metadata": {},
   "source": [
    "Let's import required libraries and print a message if we're not **missing packages**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3593f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.4.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3123, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3178, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3641, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3701, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/mg/nl4fflz92893yzhrmg7rwxth0000gn/T/ipykernel_24430/3765417831.py\", line 5, in <module>\n",
      "    from langchain_community.document_loaders import UnstructuredURLLoader, TextLoader, PyPDFLoader\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/langchain_community/document_loaders/__init__.py\", line 740, in __getattr__\n",
      "    module = importlib.import_module(_module_lookup[name])\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/langchain_community/document_loaders/pdf.py\", line 31, in <module>\n",
      "    from langchain_community.document_loaders.parsers.images import BaseImageBlobParser\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/langchain_community/document_loaders/parsers/images.py\", line 10, in <module>\n",
      "    from langchain_core.language_models import BaseChatModel\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/langchain_core/language_models/__init__.py\", line 112, in __getattr__\n",
      "    result = import_attr(attr_name, module_name, __spec__.parent)\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/langchain_core/_import_utils.py\", line 36, in import_attr\n",
      "    module = import_module(f\".{module_name}\", package=package)\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 31, in <module>\n",
      "    from langchain_core.language_models.base import (\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/langchain_core/language_models/base.py\", line 44, in <module>\n",
      "    from transformers import GPT2TokenizerFast  # type: ignore[import-not-found]\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/transformers/__init__.py\", line 27, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/transformers/utils/__init__.py\", line 24, in <module>\n",
      "    from .auto_docstring import (\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/transformers/utils/auto_docstring.py\", line 30, in <module>\n",
      "    from .generic import ModelOutput\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/transformers/utils/generic.py\", line 51, in <module>\n",
      "    import torch\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/opt/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported! You're good to go!\n"
     ]
    }
   ],
   "source": [
    "# Import standard libraries for file handling and text processing\n",
    "import os, pathlib, textwrap, glob\n",
    "\n",
    "# Load documents from various sources (URLs, text files, PDFs)\n",
    "from langchain_community.document_loaders import UnstructuredURLLoader, TextLoader, PyPDFLoader\n",
    "\n",
    "# Split long texts into smaller, manageable chunks for embedding\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Vector store to store and retrieve embeddings efficiently using FAISS\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Generate text embeddings using OpenAI or Hugging Face models\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
    "\n",
    "# Use local LLMs (e.g., via Ollama) for response generation\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "# Build a retrieval chain that combines a retriever, a prompt, and an LLM\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Create prompts for the RAG system\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "print(\"‚úÖ Libraries imported! You're good to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d118490f",
   "metadata": {},
   "source": [
    "## 2‚ÄØ-‚ÄØData preparation\n",
    "The goal of this step is to turn all reference documents into small chunks of text that a retriever can index and search. These documents typically come from:\n",
    "* PDF files: local documents such as policies, user manuals, or guides.\n",
    "* Web pages (HTML): online documentation, blog posts, or help articles.\n",
    "\n",
    "In this step, we perform two actions:\n",
    "* **Ingesting**: load every PDF and collect the raw text in a list named `raw_docs`.\n",
    "* **Chunking**: split each document into small, overlapping chunks so later steps can match a user query to the most relevant passage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27765d24",
   "metadata": {},
   "source": [
    "### 2.1 - Ingest source documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a516a48",
   "metadata": {},
   "source": [
    "We can use different libraries to load and process PDFs. A quick web search will show several options, each with its own strengths. In this case, we‚Äôll use PyPDFLoader from LangChain, which makes it easy to extract text from PDF files for downstream processing. To learn more about how to use it, refer to: https://python.langchain.com/docs/integrations/document_loaders/pypdfloader/\n",
    "\n",
    "Use **PyPDFLoader** to load every PDF whose filename matches `data/Everstorm_*.pdf` and collect all pages in a list called `raw_docs`. The content of these PDFs is synthetically generated for educational purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62ca4626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 81 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/Everstorm_Return_and_exchange_policy.pdf', 'data/Everstorm_Product_sizing_and_care_guide.pdf', 'data/Everstorm_Shipping_and_Delivery_Policy.pdf', 'data/Everstorm_Payment_refund_and_security.pdf']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 76 0 (offset 0)\n",
      "Ignoring wrong pointing object 80 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8 PDF pages from 4 files.\n"
     ]
    }
   ],
   "source": [
    "pdf_paths = glob.glob(\"data/Everstorm_*.pdf\")\n",
    "print(pdf_paths)\n",
    "raw_docs = []\n",
    "for path in pdf_paths:  \n",
    "    loader = PyPDFLoader(path)\n",
    "    # raw_docs.append(loader)\n",
    "    raw_docs.extend(loader.load())\n",
    "\n",
    "print(f\"Loaded {len(raw_docs)} PDF pages from {len(pdf_paths)} files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1fdc1c",
   "metadata": {},
   "source": [
    "### (Optional) 2.1 - Load web pages\n",
    "You can also pull content straight from the web. Various libraries support reading and parsing web pages directly into text, which is useful for building custom knowledge bases. One example is **UnstructuredURLLoader** from LangChain, which can extract readable content from raw HTML pages and return them in a structured format. To learn more, see: https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.url.UnstructuredURLLoader.html\n",
    "\n",
    "To practice, load each HTML page below and store the results in a list called `raw_docs`. We‚Äôve included a few sample URLs, but you can replace them with any links you prefer.\n",
    "\n",
    "For robustness, add an offline fallback in case a URL fails. In real projects, we typically cache fetched pages to disk, handle rate limits, and track fetch timestamps so content can be refreshed periodically without relying on live network calls during development. For this project, we don‚Äôt have offline HTML copies available, but you can still practice by loading any PDFs from the data/ folder using PyPDFLoader and appending them to raw_docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6206d960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLS = [\n",
    "#     # --- BigCommerce ‚Äì shipping & refunds ---\n",
    "#     \"https://developer.bigcommerce.com/docs/store-operations/shipping\",\n",
    "#     \"https://developer.bigcommerce.com/docs/store-operations/orders/refunds\",\n",
    "#     # --- Stripe ‚Äì disputes & chargebacks ---\n",
    "#     # \"https://docs.stripe.com/disputes\",  \n",
    "#     # --- WooCommerce ‚Äì REST API reference ---\n",
    "#     # \"https://woocommerce.github.io/woocommerce-rest-api-docs/v3.html\",\n",
    "# ]\n",
    "# raw_docs = []\n",
    "# try:\n",
    "#     for url in urls:\n",
    "#         loader = UnstructuredURLLoader(urls=url)\n",
    "#         docs = loader.load()\n",
    "#         raw_docs.append(raw_docs)\n",
    "#     print(f\"Fetched {len(raw_docs)} documents from the web.\")\n",
    "# except Exception as e:\n",
    "#     print(\"‚ö†Ô∏è  Web fetch failed, using offline copies:\", e)\n",
    "#     raw_docs = []\n",
    "#     ########################\n",
    "#     #### Your code here ####\n",
    "#     ########################   \n",
    "#     print(f\"Loaded {len(raw_docs)} offline documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d24497e6-9b20-4bd6-937b-08a28f34d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4722807a",
   "metadata": {},
   "source": [
    "### 2.2‚ÄØ-‚ÄØChunk the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8e506d",
   "metadata": {},
   "source": [
    "Long documents won‚Äôt work well directly with most LLMs. They can easily exceed the model‚Äôs context window, making it impossible for the model to read or reason over the full text at once. Even if they fit, processing long inputs can be inefficient and lead to weaker retrieval results.\n",
    "\n",
    "To handle this, we split large documents into smaller, overlapping chunks. Several libraries can help with text splitting, each designed to preserve structure or balance chunk size. A popular choice is `RecursiveCharacterTextSplitter` from LangChain, which splits text intelligently while keeping paragraph or sentence boundaries intact. To familiarize youself with the library, visit: https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html\n",
    "\n",
    "In this project, we‚Äôll split each document into chunks of roughly 300 tokens with a 30-token overlap using `RecursiveCharacterTextSplitter`. This overlap helps maintain continuity across chunks while ensuring each piece stays small enough for embedding and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5868bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Everstorm  Outfitters    RETURN  &  EXCHANGE  POLICY    Document  ROX-2025-05   Easy-Fit  Promise    If  your  gear  doesn‚Äôt  fit  or  just  isn‚Äôt  your  vibe,  send  it  back  within  **30  days**  of  delivery  for  a  refund  or  free  size  exchange.   Eligibility  Checklist    ‚óè  Unworn,' metadata={'producer': 'Skia/PDF m138 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Return_and_exchange_policy', 'source': 'data/Everstorm_Return_and_exchange_policy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}\n",
      "‚úÖ 42 chunks ready for embedding\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "\"\"\"\n",
    "YOUR CODE HERE (~2 lines of code)\n",
    "\"\"\"\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=30\n",
    ")\n",
    "chunks = text_splitter.split_documents(raw_docs)\n",
    "print(chunks[0])\n",
    "\n",
    "print(f\"‚úÖ {len(chunks)} chunks ready for embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "293270a6-42a3-4e52-9016-e73c0d545eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Skia/PDF m138 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Return_and_exchange_policy', 'source': 'data/Everstorm_Return_and_exchange_policy.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='and  custom-embroidered  items:  no  return  unless  defective.   Refund  Timeline    ‚óè  Warehouse  receipt  ‚Üí  inspection  ‚â§  3  business  days  ‚Üí  refund  initiates.    ‚óè  Stripe  /  Apple  Pay  5-7  banking  days;  PayPal  instant;  Shop-Pay  up  to  10.   Defect  &  Warranty  Claims  (12')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47414fcf",
   "metadata": {},
   "source": [
    "## 3¬†-Build a retriever\n",
    "\n",
    "A *retriever* lets the RAG pipeline efficiently look up small, relevant pieces of context at query‚Äëtime. This step has two parts:\n",
    "1. **Load a model to generate embeddings**: convert each text chunk from the reference documents into a fixed‚Äëlength vector that captures its semantic meaning.  \n",
    "2. **Build vector database**: store these embeddings in a vector database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04996d5a",
   "metadata": {},
   "source": [
    "### 3.1‚ÄØ- Load a model to generate embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47a36a0",
   "metadata": {},
   "source": [
    "The goal of this step is to convert each document chunk into a numerical vector (an embedding) that captures its semantic meaning. These embeddings allow our retriever to find and compare similar pieces of text efficiently.\n",
    "\n",
    "There are models trained specifically for this purpose, called embedding models. One popular example is OpenAI‚Äôs `text-embedding-3-small`, which produces high-quality embeddings that work well for retrieval and semantic search.\n",
    "\n",
    "If you prefer running everything locally, you can use smaller open-source models such as `gte-small` (77 M parameters). These local models load quickly, don‚Äôt require internet access, and are ideal for experimentation or environments without API access. However, they‚Äôre typically less powerful than hosted models.\n",
    "\n",
    "Alternatively, you can connect to an API service to access stronger models like OpenAI‚Äôs. These require setting an API key (for example, OPENAI_API_KEY) in your environment. OpenAI allows you to create a free account and sometimes offers limited trial credits for new users, but ongoing access requires a billing setup. \n",
    "\n",
    "In this exercise, we‚Äôll stick to the smaller gte-small model for simplicity and reproducibility. We'll use our imported `SentenceTransformerEmbeddings` library to load the model and use it to embed queries. To learn more about lagnchain's embedding support, visit: https://python.langchain.com/docs/integrations/text_embedding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8e3d222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64512\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # loads OPENAI_API_KEY if you store it in .env\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"OPENAI_API_KEY not set\"\n",
    "\n",
    "embedding_vector = []\n",
    "\n",
    "# Embed the sentence \"Hello world! and store it in an embedding_vector.\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "for chunk in chunks:\n",
    "    response = client.embeddings.create(\n",
    "        input=chunk.page_content,\n",
    "        model=\"text-embedding-3-small\"\n",
    "    )\n",
    "    embedding_vector.extend(response.data[0].embedding)\n",
    "    \n",
    "\n",
    "# print(response.data[0].embedding)\n",
    "print(len(embedding_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "812e6835-e135-4d4c-9a66-5dd82e9a6c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embedding_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a316dd",
   "metadata": {},
   "source": [
    "### 3.2‚ÄØ-‚ÄØBuild a vector database\n",
    "\n",
    "Once we have embeddings, we need a way to store and search them efficiently. A simple list wouldn‚Äôt scale well, especially when we have thousands of chunks and need to quickly find the most relevant ones.\n",
    "\n",
    "To solve this, we use **FAISS**, an open-source similarity search library developed by Meta. FAISS is optimized for fast nearest-neighbor search in high-dimensional spaces, making it ideal for tasks like semantic retrieval and recommendation. It‚Äôs strongly encouraged to visit their quickstart guide to understand how FAISS works and how to use it effectively: https://github.com/facebookresearch/faiss/wiki/getting-started\n",
    "\n",
    "In this step, we‚Äôll feed all our document embeddings into FAISS, which builds an in-memory vector index. This index allows us to efficiently query for the *k* most similar chunks to any given question.\n",
    "\n",
    "During inference, we‚Äôll use this index to retrieve the top-k relevant chunks and pass them to the LLM as context, enabling it to answer questions grounded in our documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b04c388-78a7-47ab-a358-ae968cb1770b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__builtin__', '__builtins__', '_ih', '_oh', '_dh', 'In', 'Out', 'get_ipython', 'exit', 'quit', 'open', '_', '__', '___', '__session__', '_i', '_ii', '_iii', '_i1', 'os', 'pathlib', 'textwrap', 'glob', 'UnstructuredURLLoader', 'TextLoader', 'PyPDFLoader', 'RecursiveCharacterTextSplitter', 'FAISS', 'OpenAIEmbeddings', 'HuggingFaceEmbeddings', 'SentenceTransformerEmbeddings', 'Ollama', 'ConversationalRetrievalChain', 'PromptTemplate', '_i2', 'pdf_paths', 'raw_docs', 'path', 'loader', '_i3', '_3', '_i4', '_i5', '_i6', 'chunks', 'text_splitter', '_i7', '_7', '_i8', 'load_dotenv', 'embedding_vector', 'OpenAI', 'client', 'chunk', 'response', '_i9', '_i10', 'np', 'd', 'nb', 'nq', 'xb', 'xq', '_10', '_i11', 'texts', 'metadatas', '_i12', '_12', '_i13', '_i14', '_14', '_i15', '_15', '_i16'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(42, None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts), print(globals().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd054a4a-8dd9-4c15-8ddb-2eaa0ad930a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if \"embedding_vector\" in globals():\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd280925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mg/nl4fflz92893yzhrmg7rwxth0000gn/T/ipykernel_24430/1303917573.py:34: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding_fn = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector store with 42 embeddings\n"
     ]
    }
   ],
   "source": [
    "# Expected steps:\n",
    "    # 1. Build the FAISS index from the list of document chunks and their embeddings.\n",
    "    # 2. Create a retriever object with a suitable k value (e.g., 8).\n",
    "    # 3. Save the vector store locally (e.g., under \"faiss_index\").\n",
    "    # 4. Print a short confirmation showing how many embeddings were stored.\n",
    "dim = 1536\n",
    "# text and metadata from chunks\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "metadatas = [doc.metadata for doc in chunks]\n",
    "\n",
    "# Make sure embeddings are in the right shape: List[List[float]] of length 42,\n",
    "#    each inner vector length 1536 (for text-embedding-3-small)\n",
    "# If you currently have one flat list of length 64512, reshape it.\n",
    "\n",
    "if \"embedding_vector\" in globals() and isinstance(embedding_vector, list) and len(embedding_vector) == len(texts) * dim:\n",
    "    # here the embeddings are divided into 42 list 42 of dimension 1536\n",
    "    embeddings = [embedding_vector[i*dim:(i+1)*dim] for i in range(len(texts))]\n",
    "    \n",
    "elif \"embeddings\" in globals() and isinstance(embeddings, list) and len(embeddings) == len(texts):\n",
    "    # assume it's already correct\n",
    "    pass\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"Embeddings shape mismatch. Expected either:\\n\"\n",
    "        f\"- a flat list length {len(texts)*dim}, or\\n\"\n",
    "        f\"- a list of {len(texts)} vectors.\"\n",
    "    )\n",
    "\n",
    "# sanity check\n",
    "assert len(embeddings) == len(texts)\n",
    "assert len(embeddings[0]) == dim\n",
    "\n",
    "# for future retrieval queries\n",
    "embedding_fn = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Build FAISS vectorstore using precomputed embeddings\n",
    "text_embeddings = list(zip(texts, embeddings))\n",
    "vectordb = FAISS.from_embeddings(\n",
    "    text_embeddings=text_embeddings,\n",
    "    embedding=embedding_fn,\n",
    "    metadatas=metadatas,\n",
    ")\n",
    "\n",
    "# creating retriever\n",
    "\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 8})\n",
    "\n",
    "# saving index locally\n",
    "\n",
    "vectordb.save_local(\"faiss_index\")\n",
    "\n",
    "print(\"‚úÖ Vector store with\", vectordb.index.ntotal, \"embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237c13f2-dad2-4a51-8956-8c1bd81df256",
   "metadata": {},
   "source": [
    "- note:\n",
    "\n",
    "- FAISS.from_embeddings(...) uses your precomputed vectors for documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a309bc",
   "metadata": {},
   "source": [
    "## 4¬†-¬†Build the generation engine\n",
    "At the core of any RAG system lies an **LLM**. The retriever finds relevant information, and the LLM uses that information to generate coherent, context-aware responses.\n",
    "\n",
    "In this project, we‚Äôll use **Gemma 3* (1B), a small but capable open-weight model, and run it entirely on your local machine using Ollama. This means you won‚Äôt need API keys or internet access to generate responses once the model is downloaded.\n",
    "\n",
    "**What is Ollama?**\n",
    "\n",
    "Ollama is a lightweight runtime for managing and serving open-weight LLMs locally. It provides:\n",
    "* A simple REST API running at localhost:11434, so your code can interact with the model via standard HTTP calls.\n",
    "* A model registry and command-line tool** to pull, run, and manage models easily.\n",
    "* Support for a wide variety of models (e.g., Gemma, Llama, Mistral, Phi, etc.), making it ideal for experimentation.\n",
    "\n",
    "To learn more about Ollama, visit: https://github.com/ollama/ollama. You can browse all supported models and their sizes here: https://ollama.com/library\n",
    "\n",
    "\n",
    "### 4.1 - Install `ollama` and serve `gemma3`\n",
    "\n",
    "Follow these steps to set up Ollama and start the model server:\n",
    "\n",
    "**1 - Install**\n",
    "```bash\n",
    "# macOS (Homebrew)\n",
    "brew install ollama\n",
    "# Linux\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "```\n",
    "\n",
    "If you‚Äôre on Windows, install using the official installer from https://ollama.com/download.\n",
    "\n",
    "**2 - Start the Ollama server (keep this terminal open)**\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "This command launches a local server at http://localhost:11434, which will stay running in the background.\n",
    "\n",
    "\n",
    "**3 - Pull the Gemma mode (or the model of your choice) in a new terminal**\n",
    "```bash\n",
    "ollama pull gemma3:1b\n",
    "```\n",
    "\n",
    "This downloads the 1B version of Gemma 3, a compact model suitable for running on most modern laptops. Once downloaded, Ollama will automatically handle model loading and caching.\n",
    "\n",
    "\n",
    "After this setup, your system is ready to generate responses locally using the Gemma model through the Ollama API.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d466f2",
   "metadata": {},
   "source": [
    "### 4.2 - Test an LLM with a random prompt (Sanity check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75a7987b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of today, November 2, 2023, Everstorm's return window is **late 2023**.\n",
      "\n",
      "Here's a breakdown of what we know:\n",
      "\n",
      "* **Initial Announcement:** Everstorm announced their return in early October 2023.\n",
      "* **Timeline:** They've been working on a phased return, initially focusing on a smaller, more focused release.\n",
      "* **Current Status:** They've been steadily releasing updates and improvements, indicating they're moving towards a full return.\n",
      "* **Estimated Date:** The most recent information suggests a potential release window around **late 2023**, possibly sometime in **November or December**.\n",
      "\n",
      "**Important Note:**  This is based on the most recent information available.  It's always a good idea to check the official Everstorm website and social media channels for the most up-to-date details.\n",
      "\n",
      "**Where to Find the Most Accurate Information:**\n",
      "\n",
      "* **Official Everstorm Website:** [https://everstorm.com/](https://everstorm.com/)\n",
      "* **Twitter:** [https://twitter.com/Everstorm](https://twitter.com/Everstorm)\n",
      "* **Discord:** [https://discord.gg/Everstorm](https://discord.gg/Everstorm)\n",
      "\n",
      "To help me give you even more precise information, could you tell me:\n",
      "\n",
      "*   **What specifically are you interested in knowing about the return window?** (e.g., are you wondering about a specific feature, or just the general timeline?)\n"
     ]
    }
   ],
   "source": [
    "# Expected steps:\n",
    "    # 1. Initialize the model (for example, gemma3:1b) with a low temperature such as 0.1 for more factual outputs.\n",
    "    # 2. Use llm.invoke() with a short test prompt and print the response to verify that the model runs successfully.\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"gemma3:1b\",\n",
    "    temperature=0.1,   # low temperature ‚Üí more factual / deterministic\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"What is Everstorm's return window?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43520ea3",
   "metadata": {},
   "source": [
    "## Build a RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97fa237",
   "metadata": {},
   "source": [
    "### 5.1‚ÄØ-‚ÄØDefine a system prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba1c3ce",
   "metadata": {},
   "source": [
    "At this stage, we need to tell the model how to behave when generating answers. The **system prompt** acts as the model‚Äôs rulebook. It should clearly instruct the model to answer only using the retrieved context and to admit when it doesn‚Äôt know the answer. This helps prevent hallucination and keeps the responses grounded in the provided documents.\n",
    "\n",
    "In general, a good RAG prompt emphasizes three things: stay within context, stay factual, and stay concise. This is important because RAG works by grounding the LLM in retrieved text. If the prompt is vague, the model may invent details. A precise system prompt reduces hallucinations and keeps answers aligned with your corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f7125842",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_TEMPLATE = \"\"\"\n",
    "You are a **Customer Support Chatbot**. Use only the information in CONTEXT to answer.\n",
    "If the answer is not in CONTEXT, respond with ‚ÄúI'm not sure from the docs.‚Äù\n",
    "\n",
    "Rules:\n",
    "1) Use ONLY the provided <context> to answer.\n",
    "2) If the answer is not in the context, say: \"I don't know based on the retrieved documents.\"\n",
    "3) Be concise and accurate. Prefer quoting key phrases from the context.\n",
    "4) When possible, cite sources as [source: source] using the metadata.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "USER:\n",
    "{question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1fb73b12-07b8-4f9c-91be-6b5b4c63196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it with a better system prompt\n",
    "# SYSTEM_TEMPLATE = \"\"\"\n",
    "# You are a Customer Support Chatbot for Everstorm Outfitters.\n",
    "\n",
    "# You MUST answer using ONLY the information inside <context>.\n",
    "# If the answer is not explicitly stated in <context>, reply exactly:\n",
    "# \"I don't know based on the retrieved documents.\"\n",
    "\n",
    "# Do NOT:\n",
    "# - use outside knowledge\n",
    "# - guess dates like \"as of today\"\n",
    "# - invent policies or numbers\n",
    "\n",
    "# When you answer:\n",
    "# - be concise (2-6 sentences)\n",
    "# - quote 1-2 key phrases from <context> when helpful\n",
    "# - include citations at the end of sentences like: [source: <source_file>, page: <page>]\n",
    "\n",
    "# <context>\n",
    "# {context}\n",
    "# </context>\n",
    "\n",
    "# Question: {question}\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d0d54a",
   "metadata": {},
   "source": [
    "### 5.2 Create a RAG chain\n",
    "Now that we have a retriever, a prompt, and a language model, we can connect them into a single RAG pipeline. The retriever finds the most relevant chunks from our vector index, the prompt injects those chunks into the system message, and the LLM uses that context to produce the final answer. (retriever ‚Üí prompt ‚Üí model)\n",
    "\n",
    "This connection is handled through LangChain‚Äôs `ConversationalRetrievalChain`, which combines retrieval and generation. To familiarize yourself with the library, visit: https://python.langchain.com/api_reference/langchain/chains/langchain.chains.conversational_retrieval.base.ConversationalRetrievalChain.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d69b6ae-8ad1-42b7-8810-4fc43f040a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x1242f44d0>, search_kwargs={'k': 8})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1254114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected steps:\n",
    "    # 1. Create a PromptTemplate that uses the SYSTEM_TEMPLATE you defined earlier, with input variables for \"context\" and \"question\".\n",
    "    # 2. Initialize your LLM using Ollama with the gemma3:1b model and a low temperature (e.g., 0.1) for reliable, grounded responses.\n",
    "    # 3. Build a ConversationalRetrievalChain by combining the LLM, the retriever, and your custom prompt and name it \"chain\".\n",
    "\n",
    "# 1. prompt template using SYSTEM_TEMPLATE\n",
    "prompt = PromptTemplate(\n",
    "    template=SYSTEM_TEMPLATE,\n",
    "    input_variables=[\"context\", \"question\"],\n",
    ")\n",
    "# 2. Initialize llm\n",
    "llm = Ollama(\n",
    "    model=\"gemma3:1b\",\n",
    "    temperature=0.1,\n",
    "    base_url=\"http://localhost:11434\",\n",
    ")\n",
    "\n",
    "# 3. ConversationalRetrievalChain\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,  # we created this from FAISS vectordb\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9982b152",
   "metadata": {},
   "source": [
    "When you ask a question, the retriever pulls the top few relevant text chunks, the model reads them through the system prompt, and then it generates an answer based on that context.\n",
    "\n",
    "This structure makes the system transparent and easy to debug. You can inspect what text was retrieved, tune parameters like k, and experiment with different prompts to see how they affect the output quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8edd3d",
   "metadata": {},
   "source": [
    "### 5.3‚ÄØ-‚ÄØValidate the RAG chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430d5fcc",
   "metadata": {},
   "source": [
    "We run a few questions to make sure everything behaves as expecte. Experiment by adding you own questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24dfafe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "? Question\n",
      "If I'm not happy with my purchase, what is your refund policy and how do I start a return?\n",
      "\n",
      "üí¨ Answer\n",
      "If you‚Äôre not happy with your purchase, you can initiate a return within 30 days of delivery. First, visit everstorm.example/returns and enter your order number and email. Select ‚ÄúRefund‚Äù or ‚ÄúExchange.‚Äù We‚Äôll print a prepaid label, pack securely, and the carrier will scan the box to initiate the refund process. Refunds are issued the same day your return is scanned. Banks typically post credit within 5-7 banking days, and PayPal typically offers immediate credit. Klarna typically takes 2-5 days to post the credit. You can find your Klarna statement by logging into your Klarna account.\n",
      "\n",
      "? Question\n",
      "How long will delivery take for a standard order, and where can I track my package once it ships?\n",
      "\n",
      "üí¨ Answer\n",
      "Standard orders ship within 6 orders. Tracking links are emailed upon label creation. You can track your package at [https://www.everstorm.example/tracking](https://www.everstorm.example/tracking).\n",
      "\n",
      "? Question\n",
      "What's the quickest way to contact your support team, and what are your operating hours?\n",
      "\n",
      "üí¨ Answer\n",
      "You can contact our support team via chat at 08:00‚Äì18:00 MT.  We accept Visa, Mastercard, AmEx, Discover, Apple Pay, Google Pay, PayPal, Shop Pay, and custom-embroidered items.  Installs (US only) and Klarna/Pay-in-4 are also accepted.  Refunds take up to 3 business days after scanning your return.\n"
     ]
    }
   ],
   "source": [
    "test_questions = [\n",
    "    \"If I'm not happy with my purchase, what is your refund policy and how do I start a return?\",\n",
    "    \"How long will delivery take for a standard order, and where can I track my package once it ships?\",\n",
    "    \"What's the quickest way to contact your support team, and what are your operating hours?\",\n",
    "]\n",
    "\n",
    "# Expected steps:\n",
    "    # 1. Initialize an empty chat_history list.\n",
    "    # 2. Loop through test_questions, pass each question and the current chat history to the chain, and append the new answer.\n",
    "    # 3. Print each question and the LLM's response to verify it‚Äôs working correctly.\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "#  loop through test questions\n",
    "for q in test_questions:\n",
    "    result = chain({\n",
    "        \"question\": q,\n",
    "        \"chat_history\": chat_history,\n",
    "    })\n",
    "\n",
    "    print(\"\\n? Question\")\n",
    "    print(q)\n",
    "    print(\"\\nüí¨ Answer\")\n",
    "    print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b4cf12",
   "metadata": {},
   "source": [
    "### 6‚ÄØ-‚ÄØBuild the Streamlit UI (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7604912",
   "metadata": {},
   "source": [
    "The goal here is to create a tiny demo so you can interact with your RAG system. The focus is not on UI design. We will build a very small interface only to demonstrate the end-to-end flow.\n",
    "\n",
    "There are many ways to make a UI. Some frameworks are powerful but take longer to set up, while others are simple and good for quick experiments. Streamlit is a common choice for fast prototyping because it lets you make a usable interface with only a few lines of Python. If you want to learn the basics, see the Streamlit Quickstart: https://docs.streamlit.io/deploy/streamlit-community-cloud/get-started/quickstart\n",
    "\n",
    "This step is optional. If it is not useful for your work, you can skip it. We will also complete this part together during the live session.\n",
    "\n",
    "In this cell, we write a minimal **`app.py`** that starts a simple chat UI and calls your RAG chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f98b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "YOUR CODE HERE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f220c33",
   "metadata": {},
   "source": [
    "Run `streamlit run app.py` from your terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e64b62",
   "metadata": {},
   "source": [
    "## üéâ Congratulations!\n",
    "\n",
    "You‚Äôve just built, tested, and demoed a fully working **customer-support chatbot**.  \n",
    "In one project you:\n",
    "\n",
    "* **Prepared policy docs**: loaded and chunked them for fast retrieval.  \n",
    "* **Built a vector store**: created a FAISS index with text embeddings.  \n",
    "* **Plugged in an LLM**: wrapped Gemma3 with LangChain and a prompt-aware RAG chain.  \n",
    "* **Validated end-to-end**: answered refund, shipping, and contact questions with confidence.  \n",
    "\n",
    "Swap in new documents, tweak the prompt, and your store‚Äôs customers get instant, accurate answers.\n",
    "\n",
    "üëè **Great job!** Take a moment to celebrate. The skills you used here power most RAG-based chatbots you see everywhere.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3571a54a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
